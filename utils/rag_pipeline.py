from langchain_core.prompts import PromptTemplate
from langchain_google_vertexai import VertexAI
from utils.vector_store import query_vector_store
import logging
import os
import hashlib
import re
import yaml
from pathlib import Path

# Initialize logging
logger = logging.getLogger(__name__)

class RAGPipeline:
    def __init__(self, config=None, llm=None):
        try:
            # Load config if not provided or if a Path object is provided
            if config is None or isinstance(config, (str, Path)):
                if isinstance(config, str):
                    config_path = Path(config)
                elif isinstance(config, Path):
                    config_path = config
                else:
                    config_path = Path(__file__).parent.parent / "config" / "config.yaml"

                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)

            self.config = config

            # Initialize LLM if not provided
            if llm is None:
                logger.info("Initializing Vertex AI LLM")
                # Set up Google credentials from config
                credentials_path = config.get('google_credentials', {}).get('path')
                if credentials_path and os.path.exists(credentials_path):
                    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path

                llm = VertexAI(
                    model_name=config.get('google_credentials', {}).get('model_name', 'gemini-2.0-flash-exp'),
                    project=config.get('google_credentials', {}).get('project_id'),
                    location=config.get('google_credentials', {}).get('location', 'us-central1'),
                    temperature=config.get('ai', {}).get('temperature', 0.7),
                    max_output_tokens=config.get('ai', {}).get('max_tokens', 4096)
                )

            self.llm = llm
            logger.info("Initializing RAG pipeline with Vertex AI")
            logger.info("RAG pipeline initialized successfully with Vertex AI")
        except Exception as e:
            logger.error(f"Error initializing RAG pipeline: {str(e)}")
            raise

    def get_prompt_template(self, language):
        # Define language-specific instructions with much stronger emphasis
        if language.lower() == "hindi":
            language_instruction = """
            üö® CRITICAL: ‡§Ü‡§™‡§ï‡•ã ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§π‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§®‡§æ ‡§π‡•à‡•§ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§® ‡§≤‡§ø‡§ñ‡•á‡§Ç‡•§
            üö® MANDATORY: Your entire response MUST be in Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä) language ONLY. Do NOT use English.
            üö® ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï: ‡§∏‡§≠‡•Ä ‡§∂‡§¨‡•ç‡§¶, ‡§µ‡§æ‡§ï‡•ç‡§Ø, ‡§î‡§∞ ‡§Ö‡§®‡•Å‡§≠‡§æ‡§ó ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è‡•§
            """
            section_headers = """
            ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ:
            [‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§™‡§∞‡§ø‡§ö‡§Ø 2-3 ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç]

            **‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§ ‡§î‡§∞ ‡§¶‡§∞‡•ç‡§∂‡§®** üïäÔ∏è
            [3-5 ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä]

            **‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§î‡§∞ ‡§∂‡§æ‡§∏‡•ç‡§§‡•ç‡§∞** üìú
            [‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§™‡•É‡§∑‡•ç‡§†‡§≠‡•Ç‡§Æ‡§ø ‡§î‡§∞ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ó‡•ç‡§∞‡§Ç‡§•]

            **‡§ú‡§ø‡§ú‡•ç‡§û‡§æ‡§∏‡•Å ‡§π‡•à‡§Ç? ‡§á‡§® ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç:** üí´
            [4-5 ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§µ‡§ø‡§∑‡§Ø]

            ‡§∏‡•ç‡§∞‡•ã‡§§:
            [‡§´‡§º‡§æ‡§á‡§≤ ‡§®‡§æ‡§Æ ‡§î‡§∞ ‡§™‡•á‡§ú ‡§®‡§Ç‡§¨‡§∞]
            """
        elif language.lower() == "gujarati":
            language_instruction = """
            üö® CRITICAL: ‡™§‡™Æ‡™æ‡™∞‡´á ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä‡™Æ‡™æ‡™Ç ‡™ú ‡™ú‡™µ‡™æ‡™¨ ‡™Ü‡™™‡™µ‡™æ‡™®‡´ã ‡™õ‡´á‡•§ ‡™Ö‡™Ç‡™ó‡´ç‡™∞‡´á‡™ú‡´Ä‡™Æ‡™æ‡™Ç ‡™¨‡™ø‡™≤‡™ï‡´Å‡™≤ ‡™® ‡™≤‡™ñ‡´ã‡•§
            üö® MANDATORY: Your entire response MUST be in Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä) language ONLY. Do NOT use English.
            üö® ‡™Ü‡™µ‡™∂‡´ç‡™Ø‡™ï: ‡™§‡™Æ‡™æ‡™Æ ‡™∂‡™¨‡´ç‡™¶‡´ã, ‡™µ‡™æ‡™ï‡´ç‡™Ø‡´ã ‡™Ö‡™®‡´á ‡™µ‡™ø‡™≠‡™æ‡™ó ‡™∂‡´Ä‡™∞‡´ç‡™∑‡™ï‡´ã ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä‡™Æ‡™æ‡™Ç ‡™π‡´ã‡™µ‡™æ ‡™ú‡´ã‡™à‡™è‡•§
            """
            section_headers = """
            ‡™Æ‡™æ‡™≥‡™ñ‡´Å‡™Ç:
            [‡™∏‡™Ç‡™ï‡´ç‡™∑‡™ø‡™™‡´ç‡™§ ‡™™‡™∞‡™ø‡™ö‡™Ø 2-3 ‡™µ‡™æ‡™ï‡´ç‡™Ø‡´ã‡™Æ‡™æ‡™Ç]

            **‡™Æ‡´Å‡™ñ‡´ç‡™Ø ‡™∏‡™ø‡™¶‡´ç‡™ß‡™æ‡™Ç‡™§‡´ã ‡™Ö‡™®‡´á ‡™´‡™ø‡™≤‡™∏‡´Ç‡™´‡´Ä** üïäÔ∏è
            [3-5 ‡™Æ‡´Å‡™ñ‡´ç‡™Ø ‡™∏‡™ø‡™¶‡´ç‡™ß‡™æ‡™Ç‡™§‡´ã‡™®‡´Ä ‡™Ø‡™æ‡™¶‡´Ä]

            **‡™ê‡™§‡™ø‡™π‡™æ‡™∏‡™ø‡™ï ‡™∏‡™Ç‡™¶‡™∞‡´ç‡™≠ ‡™Ö‡™®‡´á ‡™∂‡™æ‡™∏‡´ç‡™§‡´ç‡™∞‡´ã** üìú
            [‡™ê‡™§‡™ø‡™π‡™æ‡™∏‡™ø‡™ï ‡™™‡´É‡™∑‡´ç‡™†‡™≠‡´Ç‡™Æ‡™ø ‡™Ö‡™®‡´á ‡™Æ‡™π‡™§‡´ç‡™µ‡™™‡´Ç‡™∞‡´ç‡™£ ‡™ó‡´ç‡™∞‡™Ç‡™•‡´ã]

            **‡™ú‡™ø‡™ú‡´ç‡™û‡™æ‡™∏‡´Å ‡™õ‡´ã? ‡™Ü ‡™Æ‡´Å‡™ñ‡´ç‡™Ø ‡™ï‡´ç‡™∑‡´á‡™§‡´ç‡™∞‡´ã‡™®‡´Å‡™Ç ‡™Ö‡™®‡´ç‡™µ‡´á‡™∑‡™£ ‡™ï‡™∞‡´ã:** üí´
            [4-5 ‡™∏‡™Ç‡™¨‡™Ç‡™ß‡™ø‡™§ ‡™µ‡™ø‡™∑‡™Ø‡´ã]

            ‡™∏‡´ç‡™∞‡´ã‡™§:
            [‡™´‡™æ‡™á‡™≤‡™®‡´Å‡™Ç ‡™®‡™æ‡™Æ ‡™Ö‡™®‡´á ‡™™‡´á‡™ú ‡™®‡™Ç‡™¨‡™∞]
            """
        else:  # English
            language_instruction = """
            üö® CRITICAL: You must answer in English language ONLY.
            """
            section_headers = """
            Structure your answer with these sections:

            [Brief 2-3 sentence introduction with key bolded terms]

            **Core Principles and Philosophy** üïäÔ∏è
            [List 3-5 key principles with brief explanations]

            **Historical Context and Scriptures** üìú
            [Brief historical background, key figures, and important texts]

            **Curious to Learn More? Explore These Key Areas:** üí´
            [4-5 related topics as bullet points]

            **Source:**
            [Filename and page ranges]
            """

        return PromptTemplate(
            input_variables=["context", "question"],
            template=f"""{language_instruction}

            You are answering based on a large collection of Jain religious texts and books.
            Use the following context to provide a clear, concise, and engaging answer with emojis.

            IMPORTANT INSTRUCTIONS:
            1. Keep the answer concise and easy to read
            2. Use markdown formatting with ** for bold text and emojis to make it engaging
            3. Structure the answer with clear sections using emojis
            4. Cite sources at the end
            5. Focus on the core information first, then provide deeper insights

            {section_headers}

            FORMATTING RULES:
            - Use ** for bold text (e.g., **Ahimsa**, **Moksha**)
            - Add relevant emojis to section headers (üïäÔ∏è, üìú, üìö, üí´, etc.)
            - Keep paragraphs short (2-3 sentences max)
            - Use bullet points for lists
            - Make it engaging and easy to scan

            Context from multiple books:
            {{context}}

            Question:
            {{question}}

            {language_instruction}"""
        )

    def generate_unique_key(self, doc):
        metadata = doc.metadata or {}
        page_num = metadata.get('page_number', 'Unknown Page')
        source = metadata.get('source', 'Unknown Source')
        content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()[:8]
        return f"{source}_{page_num}_{content_hash}"

    def consolidate_sources(self, unique_docs):
        """Consolidate sources by PDF filename and page numbers, prioritizing most relevant sources."""
        sources_dict = {}
        for doc in unique_docs:
            if hasattr(doc, 'metadata') and doc.metadata:
                source = doc.metadata.get('source', 'Unknown Source')
                page_num = doc.metadata.get('page_number', 'Unknown Page')

                if source not in sources_dict:
                    sources_dict[source] = {'pages': set(), 'count': 0}

                if page_num != 'Unknown Page' and str(page_num).isdigit():
                    sources_dict[source]['pages'].add(int(page_num))

                sources_dict[source]['count'] += 1

        # Sort sources by relevance (count of matching chunks)
        sorted_sources = sorted(sources_dict.items(), key=lambda x: x[1]['count'], reverse=True)

        # Format the sources - prioritize the most relevant source
        formatted_sources = []
        # Only show the top 1 most relevant source to keep it clean
        for source, data in sorted_sources[:1]:
            pages = data['pages']
            if pages:
                sorted_pages = sorted(pages)
                if len(sorted_pages) > 3:
                    page_str = f"{sorted_pages[0]}, {sorted_pages[1]}, ..., {sorted_pages[-1]}"
                else:
                    page_str = ", ".join(map(str, sorted_pages))
                formatted_sources.append(f"{source} ({page_str})")
            else:
                formatted_sources.append(f"{source}")

        return formatted_sources

    def extract_related_topics(self, answer):
        """Extract related topics from the answer text"""
        # Try multiple patterns to extract topics
        topics_patterns = [
            r'Curious to Learn More\? Explore These Key Areas:\s*\*\*\s*(.*?)(?=\n\n|\*\*Source:|\*\*Here are|$)',
            r'Curious to Learn More\? Explore These Key Areas:\s*(.*?)(?=\n\n\*\*|\*\*Source:|Source:|$)',
            r'‡§ú‡§ø‡§ú‡•ç‡§û‡§æ‡§∏‡•Å ‡§π‡•à‡§Ç\? ‡§á‡§® ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç:\s*üí´\s*(.*?)(?=\n\n|$)',
        ]

        for pattern in topics_patterns:
            topics_section = re.search(pattern, answer, re.DOTALL)
            if topics_section:
                topics_text = topics_section.group(1).strip()
                # Extract topics - handle both bullet points and plain text
                topics = []
                for line in topics_text.split('\n'):
                    line = line.strip()
                    # Remove markdown bullets, asterisks, and numbering
                    line = re.sub(r'^[\*\-\‚Ä¢]\s*', '', line)
                    line = re.sub(r'^\d+\.\s*', '', line)
                    if line and len(line) > 3 and not line.startswith('**'):
                        topics.append(line)

                if topics:
                    return topics[:5]  # Limit to top 5 topics

        return []

    def get_context_summary(self, sect, language):
        """Get summary of available content for a specific sect and language"""
        try:
            # This is a placeholder implementation
            # In a real implementation, this would query the vector store
            return {
                'has_content': False,
                'sect': sect,
                'language': language,
                'document_count': 0,
                'chunk_count': 0
            }
        except Exception as e:
            logger.error(f"Error getting context summary: {str(e)}")
            return {
                'has_content': False,
                'sect': sect,
                'language': language,
                'error': str(e)
            }

    def answer_question(self, question, vector_store=None, language='english', pdf_paths=None, sect=None, user_context=None):
        try:
            logger.info(f"Processing question: {question} in language: {language}")

            # Check if vector store is available
            if vector_store is None:
                logger.warning("No vector store provided, cannot answer question")
                return self._no_content_response(question, language)

            # Retrieve relevant documents with metadata (no limit - get top 50 most relevant)
            # This supports large-scale book collections (10-15 books with ~2000 pages each)
            docs = query_vector_store(question, vector_store, k=50)
            logger.info(f"Retrieved {len(docs)} documents from vector store")

            # Check if we have any relevant documents
            if not docs or len(docs) == 0:
                logger.warning("No relevant documents found in vector store")
                return self._no_content_response(question, language)

            # Deduplicate documents using a unique key including content hash
            unique_docs = []
            seen = set()
            for doc in docs:
                unique_key = self.generate_unique_key(doc)
                if unique_key not in seen:
                    seen.add(unique_key)
                    unique_docs.append(doc)
            logger.info(f"After deduplication, {len(unique_docs)} unique documents remain")

            # Combine all documents into context with metadata
            # Group by source book for better organization
            context_by_book = {}
            for doc in unique_docs:
                if hasattr(doc, 'metadata') and doc.metadata:
                    source = doc.metadata.get('source', 'Unknown Source')
                    page_num = doc.metadata.get('page_number', 'Unknown Page')
                    content = doc.page_content

                    if source not in context_by_book:
                        context_by_book[source] = []

                    context_by_book[source].append({
                        'content': content,
                        'page': page_num
                    })
                    logger.info(f"Context content from {source} (page {page_num}): {content[:100]}...")

            # Build organized context from all books
            context_parts = []
            for source, chunks in context_by_book.items():
                pages = [c['page'] for c in chunks]
                content_list = [c['content'] for c in chunks]
                context_parts.append(
                    f"=== From: {source} (Pages: {', '.join(map(str, pages))}) ===\n"
                    + "\n\n".join(content_list)
                )

            context_text = "\n\n".join(context_parts) if context_parts else "No relevant context available."

            # Get the appropriate prompt template based on language
            prompt_template = self.get_prompt_template(language)
            prompt = prompt_template.format(context=context_text, question=question)

            # Generate answer using Vertex AI
            logger.info("Generating answer with Vertex AI")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)

            # Process the answer to ensure proper formatting
            answer = self._clean_answer(answer)

            # Extract related topics for button generation
            related_topics = self.extract_related_topics(answer)

            # Append consolidated sources only if unique_docs exist
            sources = []
            if unique_docs:
                consolidated_sources = self.consolidate_sources(unique_docs)
                if consolidated_sources:
                    # Remove any existing source lines (English, Hindi, Gujarati)
                    answer = re.sub(r'\n\*\*Source:\*\*.*?(?=\n\n|$)', '', answer, flags=re.MULTILINE | re.DOTALL)
                    answer = re.sub(r'\nSource:.*?(?=\n\n|$)', '', answer, flags=re.MULTILINE | re.DOTALL)
                    answer = re.sub(r'\n\*\*‡§∏‡•ç‡§∞‡•ã‡§§:\*\*.*?(?=\n\n|$)', '', answer, flags=re.MULTILINE | re.DOTALL)
                    answer = re.sub(r'\n‡§∏‡•ç‡§∞‡•ã‡§§:.*?(?=\n\n|$)', '', answer, flags=re.MULTILINE | re.DOTALL)
                    # Add the single most relevant source at the end
                    answer = answer.strip() + f"\n\n‡§∏‡•ç‡§∞‡•ã‡§§:\n{consolidated_sources[0]}"
                    sources = consolidated_sources

            logger.info("Answer generated successfully")
            return {
                'answer': answer,
                'key_points': [],
                'related_topics': related_topics,
                'sources': sources,
                'confidence': 0.8 if unique_docs else 0,
                'based_on_uploaded_content': bool(unique_docs)
            }
        except Exception as e:
            logger.error(f"Error generating answer: {str(e)}")
            return {
                'answer': f"Error generating answer: {str(e)}",
                'key_points': [],
                'related_topics': [],
                'sources': [],
                'confidence': 0,
                'based_on_uploaded_content': False
            }

    def _clean_answer(self, answer):
        """Clean up the answer format"""
        # Keep markdown formatting (**, emojis, etc.)
        # Only remove excessive ###
        answer = answer.replace('###', '')

        # Ensure consistent section headers
        sections = [
            'Overview', 'Historical Context', 'Content and Structure',
            'Significance', 'Key Details',
            'Curious to Learn More', 'Here are Some Related Questions', 'Source:',
            'Core Principles and Philosophy', 'Historical Context and Scriptures',
            'Sects and Key Texts'
        ]

        for section in sections:
            # Don't remove the : after section names as they may be intentional
            pass

        return answer.strip()

    def _no_content_response(self, question: str, language: str) -> dict:
        """Return a response indicating no content is available to answer the question"""
        try:
            logger.info("No content available to answer question")

            # Create language-specific "no content" messages
            no_content_messages = {
                "english": "I cannot find any information about this topic in the uploaded documents. Please upload relevant PDFs to get accurate answers.",
                "hindi": "‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§ø‡§è ‡§ó‡§è ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§á‡§∏ ‡§µ‡§ø‡§∑‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä‡•§ ‡§∏‡§ü‡•Ä‡§ï ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§æ‡§∏‡§Ç‡§ó‡§ø‡§ï ‡§™‡•Ä‡§°‡•Ä‡§è‡§´ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§",
                "gujarati": "‡™Ö‡™™‡™≤‡´ã‡™° ‡™ï‡™∞‡´á‡™≤‡™æ ‡™¶‡™∏‡´ç‡™§‡™æ‡™µ‡´á‡™ú‡´ã‡™Æ‡™æ‡™Ç ‡™Ü ‡™µ‡™ø‡™∑‡™Ø ‡™µ‡™ø‡™∂‡´á ‡™ï‡´ã‡™à ‡™Æ‡™æ‡™π‡™ø‡™§‡´Ä ‡™Æ‡™≥‡´Ä ‡™®‡™•‡´Ä. ‡™ö‡´ã‡™ï‡´ç‡™ï‡™∏ ‡™ú‡™µ‡™æ‡™¨‡´ã ‡™Æ‡´á‡™≥‡™µ‡™µ‡™æ ‡™Æ‡™æ‡™ü‡´á ‡™ï‡´É‡™™‡™æ ‡™ï‡™∞‡´Ä‡™®‡´á ‡™∏‡™Ç‡™¨‡™Ç‡™ß‡™ø‡™§ ‡™™‡´Ä‡™°‡´Ä‡™è‡™´ ‡™Ö‡™™‡™≤‡´ã‡™° ‡™ï‡™∞‡´ã."
            }

            answer = no_content_messages.get(language.lower(), no_content_messages["english"])

            logger.info("No content response returned")
            return {
                'answer': answer,
                'key_points': [],
                'related_topics': [],
                'sources': [],
                'confidence': 0,
                'based_on_uploaded_content': False
            }

        except Exception as e:
            logger.error(f"Error generating no content response: {str(e)}")
            return {
                'answer': "I apologize, but I cannot answer this question as no relevant content has been uploaded.",
                'key_points': [],
                'related_topics': [],
                'sources': [],
                'confidence': 0,
                'based_on_uploaded_content': False
            }
